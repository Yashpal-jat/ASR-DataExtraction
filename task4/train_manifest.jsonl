{"audio_filepath": "task2/super_cleaned_audios/lesson10.wav", "duration": 410.0983, "text": "algorithm and convergence multilayer perceptrons mlps representation power of mlps so welcome to lecture two of cs seven thousand and fifteen which is the course on deep learning so we will talk about mcculloch pitts neuron thresholding logic perceptrons and a learning algorithm for perceptrons and talk about the convergence of this algorithm and then we will talk about multilayer network of perceptrons and finally the representation power of perceptrons so  let us start module one which is on biological neurons so  remember during the history we had started all the way back in the one880s when we spoke about biological neurons so we will just start there spend a few minutes on it and then go on to the computational models which is mcculloch pitts neuron so now this is a course on deep learning so we are going to talk about deep neural networks now the most fundamental unit of a deep neural network is something known as an artificial neuron and the question is why is it called a neuron where does the inspiration come from so we already know that the inspiration comes from biology and more specifically it comes from the brain because we saw that way back in the one890s this term neuron was coined for neural processing units or the cells in our brain so now before we move on to the computational neurons or the artificial neurons we will just see the biological neurons in a bit more detail and then we will move on from there so this is what a typical biological neuron looks like so  here actually there are two neurons this portion is called the dendrite so it is used to receive inputs from all the other neurons so that is the place where the input comes in then remember we said that in one950s we discovered that these neurons are actually discrete cells and there is something which connects them so that connection is called a synapse and it decides the strength of the connection between these two neurons so there is an input there is some strength to the connection then once this neuron receives inputs from various other neurons it starts processing it so that is the central processing unit which is called the soma and once it is done this processing it will it is ready to send its output to other set of neurons so that output is carried on by the axon so we have inputs we have some weights attached to the input we have some processing and then an output so that is what a typical biological neuron looks like and let us see a very cartoonish illustration of how this works right how the neuron works so our sense organs interact with the outside world and then they pass on this information to the neuron and then the neuron decides whether i need to take some action in this case the action could be whether i it should laugh or not right whether the input is really funny enough to evoke laughter so  if that happens this is known as something that the neuron has fired now of course in reality it is not just a single neuron which does all this there is a massively parallel interconnected network of neurons so you see a massive network here now the neurons in the lower level site so these neurons they actually interact with the sensory organs they do some processing based on the inputs so they decide whether i should fire or not and if they fire they transmit this information to the next set of neurons and this process continues till the information is relayed all the way back and then finally you decide whether you need to take any action or not again in which this case it should be laughter so  that is how it works and when i say massively parallel interconnected network i really mean it because there are one0 raise to eleven which is roughly one00 billion neurons in the brain now this massively parallel network also ensures that there is some division of work now what do you mean by that is not that every neuron is responsible for taking care of whether i should laugh or not or not every neuron is responsible for processing visual data some neurons may possess visual data some neurons may possess speeds data and so on so there is this division of work every neuron has a certain role to play so for example in this cartoonish example that we took so there might be this one neuron which fires if the visuals are funny right whatever you are seeing is funny there will be one neuron which finds sheldons speech to be funny the way he speaks so  that might be funny and there might be another neuron which actually  finds  the dialogue content to be funny and now  all  of this  pass on the information to the next level and this guy would fire if at least two of these three inputs are funny so that means i have some threshold based on which i decide whether to react or not if it is really funny then only i laugh it otherwise i will not laugh so the neurons in the brain as was obvious in the previous slide are arranged in a hierarchy and i will take a more realistic example where we look at the visual cortex so is  this  is  the  portion  of the  brain  which  is  responsible  for processing visual information right so as you see here you have our retina from where the information starts flowing and it goes through various levels so you see you follow the arrows and you will see there are several levels there is one level here then another here another here and so on right so it is again as i was trying to illustrate in that cartoon the information is relayed through multiple layers and then it goes all the way back to the spinal cord which decides that in this case i need to move the muscle right so that is what is being decided here right so the information  flows through a hierarchy of layers and in this particular case i am going to focus on these three circled layers which are vone vtwo and ait right so these actually form a hierarchy and let us see what this hierarchy does right so at layer one you detect edges and corners so i am looking at you all i just see some dots and some shapes so that is what layer one recognizes i just recognize some edges and some dots and so on now layer two tries to group all of these together and come up with some meaningful feature groups right so it realizes oh these two edges actually form the nose these two dots actually form the eyes and these two edges actually form the mouth right so that is slightly higher level of processing that it is doing and then layer three further collects all this and leads to higher level objects right so now it is realizing all these things put together is actually a human face right so you add edges and circles or dots then you had some feature groups and then the feature groups combine into objects right so that is how this hierarchy processes so here is a disclaimer i understand very little about how the human brain works right and what you saw is a very oversimplified explanation of how the brain works right what i told you is there is an input a layer of networks which does a network which has many layers which does some processing and then you have an output right that is the very simplistic view that i gave you this is an oversimplified version but this version suffices for everything that we need for this course right this is not a biology or a neural processing course right so it is enough for this course so that is where we will end module one "}
{"audio_filepath": "task2/super_cleaned_audios/lesson9.wav", "duration": 243.5971, "text": "so lot of fields have adopted deep learning now and lot of state of the art ai systems are  based on deep neural networks but now what is needed is after all this madness were  deep learning has taken over a lot of research areas can we now bring in some sanity to  the proceeding so this is really a need for sanity and why i say that is that because there is this paradox of deep learning so there is this interesting question that why does deep learning works so well despite having a high capacity so the deep neural networks have a very high capacity which means that susceptible to over fitting so most of you would have done some course on machine learning so there you know that over fitting is bad because you are just memorizing the training data and then you might not be able to do so well and at tested and over fitting happens when your model has a high capacity so  even though deep neural networks have high capacity why are they doing so well we will focus on this high capacity but when we talk about the universal approximation theorem and give some arguments for why deep neural networks have such a high capacity the other thing is they have this numerical instability right so we spoke about these vanishing and exploding gradients  and again we will talk about this later on in the course so despite this training difficulties why is it that deep neural networks performs so well and of course they have this sharp minima which is again it could lead to over fitting so if you look at there is an  optimization problem it is not a  neat convex optimization problem so it is a non convex optimization problem so why does it still do so well so it is also not very robust so here is an example on the right hand side the figure that you show so the first figure is actually of a panda and the machine is able to detect this panda with some fifty-seven percent confidence right we have trained a machine for a lot of animal images we have shown it a lot of animal images at test time we show at this image the first image that you see on the right hand side and is able to classify this is a panda with fifty-seven percent confidence but now what i do is i add some very random noise so that second image that you see with some very random pixels if i add it to this image i will get a new image so every pixel in this image is added to this new noise image and i get the image which is see on the third the third image that you see right to you and me or to any average human this still looks like a panda there is hardly any difference between this image and the original image but now if you pass this to the machine all of a sudden instead of recognizing this is a panda it starts to recognize it as a gibbon and that too with ninety-nine percent confidence so why is it that they are not very robust and despite this not being very robust why are deep neural networks so successful so people are interested in these questions and people have started asking these questions there are no clear answers yet but slowly and steadily there is an increasing emphasis on explainability and theoretical justifications so it is not enough to say that your deep neural network works and gives you ninety-nine percent accuracy it is  also good to have an explanation for why that happens is it that some components of the networks are really able to discriminate between certain patterns and so on so what is going on inside the network which is actually making it work so well right and hopefully this will bring in some sanity to the proceedings so instead of just saying that i apply deep learning to problem x and got ninety percent success we will also make some kind of more sane arguments just to why this works and what is the further promise of this and thinks like that so  that is roughly a  quick historical recap of where deep learning started and where it is today starting all the way back from advances in biology in one thousand, eight hundred and seventy-one to recent advances till two thousand and seventeen and so on deep learning right and here are few url so you could take a look at this for a lot of interesting applications of recurrent neural networks  bunch of startups which have come up in this space is working on very varied and interesting problems and here are all the references that i have used for this particular presentation so that is where we end lecture one and i will see you again soon for lecture two "}
{"audio_filepath": "task2/super_cleaned_audios/lesson8.wav", "duration": 252.9156, "text": "so this was all happening where deep learning now started showing a lot of promise in a  lot of fields nlp vision speech and again this deep reinforcement learning and so on  which led to this complete madness starting from two thousand and thirteen well almost for every application the traditional methods were then overwritten or kind of beaten by deep neural network based system so something like language modelling which has been around since probably 1950s or so  now the reining algorithm or the better algorithm for language modelling is now something which is based on deep neural networks then similarly for speech recognition lot of work a lot of probabilistic lot of work based on probabilistic models was done in this or in the speech area or the speech literature for the past thirty forty years and now all of that has been overcome by deep neural network based solutions same for machine translation a lot of interest in this field a lot of companies now have their machine translation systems based on deep neural networks as opposed to the earlier phrase based statistical machine translations or the probabilistic models which were used earlier similarly for conversation modelling dialogue a lot of new work started in dialogue post a deep learning era where people now realize that if you have a lot of sequences of conversations you could actually try to train a deep neural network to learn from this sequence and have conversations with humans of course you are nowhere close to human level conversations we are very very far off from them but in limited domains these bots are showing some success now same for question answering where you are given a question and you want to answer it either from a knowledge graph or from a document or from a image and so on and in the field of computer vision things like object detection most of the reigning systems or the best performing systems nowadays are deep neural network based systems a lot of advances are being made on these systems over in the last few years same for visual tracking where you want to track the same person in a video or image captioning where you want to generate captions for images for example people upload a lot of images on facebook and if you want to automatically caption them or imagine you are on a reselling site right something like olx where you upload your furniture and you do not provide a description from that but can the machine already automatically generate a description for it so it is easier for the human to read what that product is and so on so similarly video captioning i given a video anyone to caption the main activity which is happening in that video all of these problems are being solved using deep learning based solutions using a combination of something known as feed forward neural networks or convolutional neural networks or recurrent neural networks and so on visual question answering you are given an image and a question and you want to answer that question video question answering answering questions from videos video summarizations if you are given a large video and you want to generate a trailer a sort of a trailer for that video contains which kind is the most important frame for that video even these systems are based on deep learning then this was all about classification recognition and so on but  now people started getting more ambitious that can we humans are very good at creativity so can we use machines to be creative right to generate images so now  if i have seen a lot of celebrity faces can i generate new celebrity faces or if i have seen a lot of bedroom images and i am if a fireman architect now can i generate new bedroom images can i can we train a machine to generate new bed bedroom images so a lot of phenomenal progress or work has happened in this field in the last four five years starting with things like generative adversarial networks variational autoencoders and so on and people  are now  starting  to  seriously invest  into  creativity  that  how  to  make machines creative again we are far off from where the desired output but there is still significant progress happening in this field generating audio so that was about generating images you can generate music also and this is again about generating images and so on "}
{"audio_filepath": "task2/super_cleaned_audios/lesson5.wav", "duration": 125.9562, "text": "so this is what the progression was right that in two thousand and six people started or the study by hinton and others led to the survival and then people started realizing the deep neural networks and actually we use for lot of practical applications and actually beat a lot of existing systems but there are still some problems and we still need to make the system more robust faster and even scale higher accuracies and so on so in parallelly while there was lot of success happening from two thousand and twelve to two thousand and sixteen or even two thousand and ten to two thousand and sixteen in parallel there will also a lot of research to find better optimization algorithms which could lead to better convergence better accuracies and again some of the older ideas which were proposed way back in one thousand, nine hundred and eighty-three now this is again something that we will do in the course so most of the things that i am talking about we are going to cover in the course so we are going to talk about the imagenet challenge we are going to talk about all those networks the winning networks that i had listed there alex net zf net google net and so on we are going to talk about nesterov gradient descent which is listed on the slide and many other better optimization methods which were proposed starting from two thousand and eleven so there was this parallel resource happening while people were getting a lot of success using traditional neural networks they are also interested in making them better and robust and lead for lead to faster convergence and better accuracies and so on so this led to a lot of interest in coming up with better optimization algorithms and there was a series of these proposed starting from two thousand and eleven so adagrad is again something that we will do in the course rms prop adam eve and many more so many new algorithms i have been proposed and in parallel a lot of other regularization techniques or  weight  initialization  strategies  have  also  been  proposed  for  example  batch normalization or xavier initialization and so on so  these are all things which were aimed at making neural networks perform even better or faster and even reach better solutions or better accuracies and so on this all that we are going to see in the course at some point or the other "}
{"audio_filepath": "task2/super_cleaned_audios/lesson4.wav", "duration": 170.2361, "text": "i will talk about the history of convolutional neural networks and i call this part of history as cats and it will become obvious why i call it so so around one thousand, nine hundred and fifty-nine hubel and wiesel did this famous experiment they are still i think you could see some videos of it on youtube where there is this cat and there was a screen in front of it and on the screen there were these lines being displayed at different locations and in different orientations so  slanted horizontal vertical and so on and there are some electrodes fitted to the cat and they were measuring trying to measure that which parts of brain actually respond to different visual stimuli let us say if you show it stimulus at a certain location does the different part of the brain fire and so on so and one of the things of outcomes of the study was that that different neurons in brain fire to only different types of stimuli it is not that all neurons in brain always fire to any kind of visual stimuli that you give to them  so this is essentially roughly the idea behind convolutional neural networks starting from something known as neocognitron which was proposed way back in one thousand, nine hundred and eighty you could think of it as a very primitive convolutional neural network i am sure that most of you have now read about or heard about convolutional neural networks but something very similar to it was proposed way back in one thousand, nine hundred and eighty and what we know as the modern convolutional neural networks maybe i think yan li kun is someone who proposed them way back in one thousand, nine hundred and eighty-nine and he was interested in using them for the task of handwritten digit recognition and this was again in the context of postal delivery services so lot of pin codes get written or phone numbers get written on the postcards and there was a requirement to read them automatically so that they can be the letters or postcards can be separated into different categories according to the postcard according to the postal code and so on right so or the pin code so that is where this interest was there and one thousand, nine hundred and eighty-nine was when this convolutional neural networks were first proposed or used for this task and then over the years several improvements were done to that and in one thousand, nine hundred and ninety-eight this now how famous data set the mnist data set which is used for teaching deep neural networks courses or even for initial experiments with various neural network based networks this is one of the popular data sets which is used in this field and this was again released way back in one thousand, nine hundred and ninety-eight and even today even for my course i use it for various assignments and so on so it is interesting that an algorithm which was inspired by an experiment on cats is today used to detect cats in videos of course among other various other things is just i am just jokingly saying this "}
{"audio_filepath": "task2/super_cleaned_audios/lesson6.wav", "duration": 353.6762, "text": "so i  was talking about successes in image speech pattern recognition even natural language processing and so on so one interesting thing here is about sequences so i will talk about sequences now  sequences are everywhere when you are dealing with data  so  you have time series which is like say the stock market trends or any other kind of a series time series then you have speech which is again a series of phonemes or you have music you have text which is a series of words you could even have videos which are the series of images right one frame each image each frame can be considered to be an image and so on so in speech data one peculiar characteristic of speech data is that every unit in the sequence interacts with other units so words on their own may not mean much but when you put them together into a sentence they all interact with each other and give meaning to the sentence right and the same can be said about music or speech or any kind of sequence data so all these elements of the sequence actually interact with each other  so there was a need for models to capture this interaction and this is very important for natural language  processing because in natural language processing you deal with sequence of words or all your texts or sentences or documents or all sequences of words so that is very important and the same in the case of speech also  so if you take up any deep learning paper nowadays it is very likely that you will come across the term recurrent neural network or lstms which are long short term memory cells and so on  so this is also something which was proposed way back in one thousand, nine hundred and eighty-six  so a recurrent neural network is something which allows you to capture the interactions between the elements of your sequence i had said at a very layman level but of course you are going to see this in much more detail in the course and this was also not something new even though you hear about it a lot in the past three to four years the first recurrent neural network and what you see here is exactly a very similar to what we are going to cover in the course was proposed way back in jordan by jordan in one thousand, nine hundred and eighty-six   its variant was proposed by elmen in 1990so this is again not a very new idea this has existed for some time but now there are various factors because of which it has been possible to now start using them for a lot of practical applications as i said one you have a lot of compute time and the other you have a lot of data and the third is now the training has stabilized a lot because of these advances which i was talking about in terms of better optimization algorithms better regularization better weight initialization and so on  so it has become very easy to train these networks for real world problems at a large scale so that is why they have become very popular and hear about them on a regular basis but it is again something which was done way back  so from one thousand, nine hundred and ninety-nine to 199four actually people also looking at various problems will be training neural networks and recurrent neural networks and so that this problem which is known as exploding and the vanishing gradient problem which is again something that we will see in the course in reasonable detail we have this problem and it is very difficult to train recurrent neural networks for longer sequences so if you have a very long sequence or a time series you cannot really train a recurrent neural network to learn something from that   and to overcome these problems around one thousand, nine hundred and ninety-seven  long short term memory cells were proposed and this is again something that we will cover in the course and this is now almost de facto standard used for training for a lot of nlp work lstm are used as one of the building blocks and another variants  of lstms which are known as gated recurrent units and some other variants  so  this  is  also  not  something  new  even  though  they  have  become  very  popular nowadays like almost any article that you pick about to talk about any article on deep learning that pick about to talk about recurrent neural networks or lstms or gated recurrent units this is not something which is new   lstms had come way back in one thousand, nine hundred and ninety-seven  but again due to various compute and other issues which i said at that time it is not so easy to use them but by 201four because of these parallel progresses which i mentioned in terms of optimization regularization and so on people are now able to use rnns lstms for large scale sequence to sequence problems and in particular a very important discovery at this time are very important model which was proposed at this time which is attention mechanism which is used in a lot of deep neural networks nowadays which enabled to deal with a lot of sequence prediction problems  for example translation where you have given one sequence in one language and you want to generate the equivalent sequence in another language so this is known as a sequence to sequence translation problem so for that people proposed a sequence to sequence attention network and this was one of the key discoveries which then led to a lot of adaptation of or  adoption of deep neural networks for nlp  a lot of research in nlp happened which was then driven by deep neutral networks so a lot of existing algorithms which are non neural network based algorithms which are traditionally used for nlp was slowly replaced by these deep neural network based algorithms ok and again this idea of attention  itself is something  that was explored earlier also somewhere around one thousand, nine hundred and ninety-one or so and it was something known as reinforcement learning which was used for learning this attention mechanism what attention basically tells you is that if you have a large sequence and if you want to do something with this sequence what are the important entities of this sequence or elements of this sequence that you need to focus on so this is again something that we will look at in detail in the course   "}
{"audio_filepath": "task2/super_cleaned_audios/lesson7.wav", "duration": 81.4357, "text": "now since i mentioned rl so we will go on to the next chapter which was now becoming much more ambitious with what you can do with deep learning and people started beating humans at their own game quite literally so there was this starting with atari games in two thousand and fifteen where resources from deep mind show that you could train a deep neural network to play atari games and do much better than what humans do so that is something that they were able to show on atari games and then people started looking at other game so then there was this go and this popular tournament and which alphago which is deep reinforcement learning based agent was actually able to beat the reigning champion at that time one of the best players of go at that time then even at poker were something known as deepstack which is again a deep reinforcement learning based agent which is able to beat eleven professional poker players at this game  then other games like defense of the ancients since on which is a much more complex strategy based game where again deep reinforcement learning based agents have shown a lot of success in beating top professional players on this game "}
{"audio_filepath": "task2/super_cleaned_audios/lesson3.wav", "duration": 435.4997, "text": "when this deep revival happened so in two thousand and six a very important study was or a very important contribution was made by hinton and salakhutdinov   sorry if i have not pronounced it properly and they found that a method for training very deep neural network effectively now again the details of these are not important we will be doing that in the course at some point but what is the important take away here is that while from one thousand, nine hundred and eighty-nine to two thousand and six we knew that there is an algorithm for training deep neural networks and they can potentially be used for solving a wide range of problems because that is what the universal approximation theorem said  but the problem was that in practice we were not being able to use it for much  it was not easy to train these networks but now with this technique there was revived interest and hope that now actually can train very deep neural networks for lot of practical problems this sparked off the interest again and then people started looking at all such of thing right that even this particular study which was done in two thousand and six  will actually be very simple to something done way back in nine thousand, one hundred and ninety-three and which again showed that you can train a very deep neural network but again due to several factors may be at that time due to the computational requirements or the data requirements or whatever i am not too sure about that it did not become so popular then but by two thousand and six probably the stage was much better for these kind of networks or techniques to succeed so then it became popular in two thousand and six   then  this  two thousand and six  to  two thousand and nine  people  started  gaining  more  and  more  insights  into  the effectiveness of this discovery made by hinton and others which is unsupervised pre training right that is what i spoke about on the previous slide unsupervised pre training  and they started getting more and more insights into how you can make deep neural networks really work so they came up with various techniques some of which we are going to study in this course so this was about how do you initialise the network better what  is  the  better  optimization  algorithm  to  use  what  is  the  better  regularization algorithm to use and so on so there were many things which were started coming out at this period two thousand and six to two thousand and nine and by two thousand and nine everyone started taking note of this and again deep neural networks of artificial neural networks started becoming popular  that is when people realised that all this all the negative things that were tied to it that you are not able to train it well and so on have slowly  people have started finding solutions to get by those and maybe we should start again focusing on the potential of deep neural networks and see if they can be used for large scale practical application so this two thousand and six to two thousand and nine was again a slow boom period were people were again trying to do a lot of work to popularize  deep neural  networks and get rid of some of the problems which existed in training them  now from two thousand and nine onwards there was this series of success is which kind of caught everyone which made everyone to stand up and take notice  right  that this is really working for a lot of practical applications starting with handwriting recognition  so around two thousand and nine these guys won handwriting recognition competition in arabic and they did way better than the competitor systems using a deep neural network and then this was a success  so this was an handwriting recognition and then there was speech so this shown that various  existing  systems  the  error  rate  of  these  system  could  be  seriously  be significantly reduced by using deep neural networks or plugging in a deep neural network component to existing systems right so this was handwriting and then speech  then again some kind of pattern recognition which was on handwritten digit recognition for mnist this is a very popular data set which had been around since ninety-eight and a new record was set on this data so this is the highest accuracy that was achieved on this data set around that time in two thousand and ten sorry and this is also the time when gpus entered the same so before that all of the stuff was being done on cpus but then people realised that very deep neural networks require a lot of computation and lot of this computation can happen very quickly on gpus as opposed to cpus  so people started using gpus for training and that drastically reduced the training and inference time  so  that was again something which sparked a lot of interest right because even though these were successful they were taking a lot of time to train but now the gpus could even take care of that and this success continued   so  people  started  gaining  or  getting  success  in  other  fields  like  visual  pattern recognition so this was a competition on recognising traffic sign boards and here again a deep neural network did way better than its other competitors   and then the most popular or one thing which made neural networks really popular was this image net challenge which was around since two thousand and eight or two thousand and nine and before two thousand and twelve when this alexnet was one of the participating systems in this competition most of the systems were non neural network based systems and this competition was basically about classifying a given image into one of thousand classes so this could be an image of a bird or a dog or a human or car truck and so on say you have to identify the right class of the main object in the image so in two thousand and twelve this alexnet which was a deep neural network or a convolutional neural network based system was able to actually outperform all the other systems by a margin of sixty-seven percent so the error for this system was sixteen percent and this is a deep neural network because it had eight layers  the next year this was improved further and something known as zf network propose which was again eight layers but it did better than alexnet the next year even a deeper network with nineteen layers was proposed which did significantly better than alexnet then google entered the scene and they proposed something which is twenty-two layers and again reduced the error then microsoft joined in and they proposed something which had one hundred and fifty-two layers and the error that you see here is actually better than what humans do  so even if a human was asked to label this image because of certain law certain noise in the image and so on even a human is bound to make more errors than thirty-six per cent that means even if you show hundred images to humans he or she is bound to may go wrong or more than three or four of these images right there is this system was able to get an error of thirty-six per cent over the large test set  so this two thousand and twelve to 20sixteen period were there was this continuous success on the image net challenge  as  well  as  successes  in  other  fields  like  natural  language  processing handwriting recognition speech and so on so this is the period where now everyone started talking about deep learning and lot of company started investing in it a lot of traditional systems which were not deep neural network based was now started people started converting them to deep neural network based system  so translation system speed systems image classification object detection and so on there were lot of success in all these fields using deep neural networks  and this particular thing that we are talking about which is image net and the success in this was driven by something known as convolutional neural networks  "}
{"audio_filepath": "task2/super_cleaned_audios/lesson2.wav", "duration": 781.7387, "text": "we will start talking about artificial intelligence and this is titled as from the spring to the winter of ai so i am going to talk about when was this boom in ai started or when is that people started thinking and talking about ai seriously and what eventually happened to the initial boom and so so  let  us start  with  one thousand, nine hundred and forty-three  whereas  i  saying  that  there  was  a  lot  of  interest  in understanding how does a human brain work and then come up with a computational or  a  mathematical  model  of  that  so  mcculloch  and  pitts  one  of  them  was  a neuroscientist and the other one was a logician no computer scientists or anything at that point of time  and they came up with this extremely simplified model that just as a brain takes a input from lot of factors so now suppose you want to decide whether you want to go out for a movie or not so you would probably think about do you really have any exams coming up that could be our factor xone you could think about is a weather good to go out is it deep learning raining would it be difficult to go out at this point would there be a lot of traffic is it a very popular movie and hence tickets may not be available and so on  so being kind of presses all this information you might also look at things like the reviews of the movie or the imdb rating of the movie and so on and based on all these complex inputs it applies some function and then takes a decision yes or no that i want to probably go for a movie  so this is an overly simplified model of how the brain works is and what this model says is that you take inputs from various sources and based on that you come up with the binary decision right so this is what they proposed in one thousand, nine hundred and forty-three so now we have come to an artificial neuron so this is not a biological neuron this is how you would implement it as a machine right so that was in one thousand, nine hundred and forty-three  then along and then this kind of led to a lot of boom in our interest in artificial intelligence and so on and i guess around one thousand, nine hundred and fifty-six in a conference the term artificial intelligence  was  a formally  coined  and  within  a  one or  two years  from  there  frank rosenberg  came  up  with  this  perceptron  model  of  doing  computations  or  what perceptron model of what an artificial neuron could be and we will talk about this in detail later on the course and not tell you what these things are as of now just think of the a new model was proposed and this is what he had to say about this model right so he said that the perceptron may eventually be able to learn make decisions and translate languages do you find anything odd about this statement yeah so learn and make decisions make sense but why translate languages why is so specific why such a specific interest in languages so that you have to connect back to history so this is also the period of the cold war and there was always always a lot of interest there was lot of research and translation was actually fuelled by the world war and evens that happened after that where these countries which were at loggerheads with each other  they wanted to understand what the other country is doing but they did not speak each other\u2019s language that is why there was a lot of interest from espionage point of view or from spying and so on to be able to translate languages and hence that specific require and lot of this research would have been funded from agencies which are interested in these things right and the defence or war or something  so and this work was largely done for the navy and this is an this is an extract from the article written in new york times way back in one957 or fifty-eight where it was mentioned that the embryo often this perceptron is an embryo of an electronic computer that the navy expects will be able to walk talk see write reproduce itself and be conscious of it is existence  so i am not quoting something from two0one7 or oneeight this is way back in one957 fifty-eight why i am that is why i like the history part of it so recently there is a lot of boom or a lot of hype around ai that ai will take over a lot of things will take our jobs it might eventually we might be colonized by ai agents and so on  so i just want to emphasize that i do not know whether that will happen or not but this is not something new we have been talking about the promise of ai as far back since one957 one9fifty-eight right this not something new that people are talking about now it is always been there and to what extent this promise will be fulfilled is yet to be seen  and of course as compared to one957fifty-eight we have made a lot of progress in other fields which have enabled ai to be much more successful than it was earlier for example we have much better compute power now we have lots of data now and all thanks to the internet and other things that you can actually crawl tons and tons of data and then try to learn something from a data or try to make the machine learn something from it so we have made a lot of progress in other aspects where which ai is now at a position where it can really make a difference but just wanted to say that these are not things which i have not been said in the past it has always been the it has always been considered to be very promising and perhaps a bit hyped also so that is about one957fifty-eight  then now what we talk about what is all the for the past eight to one0 years at least when we talk about ai talking about deep learning and that is what this course  is about largely about deep learning i am not saying that other and what deep learning is largely about if i want to tell you in a very layman nutshell term is it is about a large number of artificial neurons connected to each other in layers and functioning towards achieving certain goal so this is like a schematic of what a deep neural network or a feed forward neural network would look like now this is again not something new which is up in the last eight to one0 years although people have started discussing it a lot in the last eight to one0 years look at it way back in  one9656eight opposed something which looked very much like a modern deep neural network or a modern feed forward neural network  and in many circles he is considered to be one of the founding fathers of modern deep learning about the springtime for ai and what i mean by that that everyone was showing interest in that the government was funding a lot of research in ai and people really various applications health care defence and so on  and then around one969 an interesting paper came out by these two gentlemen minsky and papert which essentially outlined some limitations of the perceptron model and we will talk about these limitations later on in the course in the second or third lecture but for now i will not get into a details of that but what it is said that it is possible that a perceptron cannot handle some very simple functions also  so you are trying to make the perceptron learn some very complex functions because the way we decide how to watch a movie is a very complex function of the inputs that we considered but even a simple function like xor or is something which a perceptron cannot be used to model that is what this paper essentially showed and this led to severe criticism for ai and then people started losing interest in ai and lot of government funding actually subsided after one969  all the way to one9eight6 actually this was the ai winter of connectionism so there was very little interest in connectionist ai so there are two types of ai one is symbolic ai and the other is connectionist ai so whatever we are going to study in this course about neural networks and all that probably falls in connectionist ai paradigm and there was no interest in this and people i mean hard to get funding and so on for these one7 to oneeight years  and that was largely triggered by this study that was done by minsky and papert and interestingly they were also often misquoted and what they had actually said in that papers so they had said a single perceptron cannot do it they in fact said that a multi layer network of perceptrons can do it but no one focused on the second part that a multilayer network of perceptron people started pushing the idea that a perceptron cannot do it and hence we should not be investigating it and so on right so that is what happened for a long time and this known as the winter the first winter  then around one9eight6 actually came this algorithm which is known as back propagation again this is an algorithm which we are going to cover in a lot of detail in the course in the 4th or 5th lecture and this algorithm actually enables to train a deep neural network right so deep network of neurons is something that you can train using this algorithm  now this algorithm was actually popularized by at rumelhart and others in one9eight6 but it is not completely discovered by them this was also around in various other fields so it was there in i think in systems analysis or something like that it was being used for other purposes in a different context and so on and rumelhart other and others in one9eight6 were the first to kind of popularize it in the context of deep neural networks  and this was a very important discovery because even today all the neural network so most of them are trained using back propagation right and of course there have been several other advances but the core remains the same that you use back propagation to train a deep neural network right so something this was discovered almost thirty years back is still primarily used for training deep neural networks that is why this was a very important paper or breakthrough at that time  and around the same time  so  again interestingly so back propagation is used in conjunction with something known as gradient descent which was again discovered way back in oneeight47 by cauchy and he was interested in using this to compute the orbit of heavenly bodies that is something that people care about at that time today of course we use it for various other purposes one of them being discovering cats and videos or even for medical imaging or for describing whether certain have of cancer is being depicted in a xray or things like that there is a lot of other purposes for which deep neural networks enhance and hence back propagation gradient descent and other things are being used for it but again these are not very modern discoveries these are dated way back thirty years and even gradient descent is almost one50 years and so on so that is what i wanted to emphasize  and around the same time in one9ninety or one9eight9 there is this another interesting theorem which was proved which is known as the universal approximation theorem and this is again something that we will cover in the course in the third lecture or something like where we will talk about the power of a deep neural network so again the importance of this or why this theorem was important will become clear later and when we cover it in detail but for now it is important to understand that what this theorem said is that if you have a deep neural network you could basically model all types of functions continuous functions to any desired precision  so what it means in very layman terms is that if the way you make decisions using a bunch of inputs is a very very complex function of the input then you can have a neural network which will be able to learn this function right in many laymen terms that is what it means  and if i have to hype it up a bit or i have to say it in a very enthused and excited manner i would say that basically it says that deed neural networks can be used for solving all kinds of machine learning problems and that is roughly what it says but with a pinch of salt and a lot of caveats but that is what it means at least in the context of this course so this is all around one9eight9 and despite this happening some important discoveries towards the late end of eight0\u2019s which was back propagation universal approximation theorem people were still not being able to use deep neural networks for really solving large practical problems and a few challenges there was of course the compute power at that time was not at a level where it could support deep neural networks  we do not have enough data for training deep neural networks and also in terms of techniques while back propagation is a sound technique it is to fail when you have really deep neural network so when people try it training a very deep neural network they found that the training does not really converge the system does not really learn anything and so on and there were certain issues with using back propagation off the shelf at that time because of which it was not very successful so again despite these slight boom around eight6 to ninety where some important discoveries were made and even follow up in 9two ninety-three and so on there is still not a real big hype around deep neural networks or artificial neural networks and at time again a slump a slow winter right up till two006  "}
{"audio_filepath": "task2/super_cleaned_audios/lesson1.wav", "duration": 376.1425, "text": "hello everyone welcome to lecture one of cs7 0one5 which is the course on deep learning in  today\u2019s lecture is going to be a bit non technical we are not going to cover any technical concepts  or  we only going to talk about a brief or partial history of deep learning so we hear the terms artificial neural networks artificial neurons quite often these days and i just wanted you take you through the journey of where does all these originate from and this history contains several spans across several fields not just computer science we will start with biology then talk about something in physics then eventually come to computer science and so on so with that let us start so just some acknowledgments and disclaimers i have taken lot of this material from the first people which i have mentioned on the bullet and there might still be some errors because its dates as back as one87one so maybe i have got some of the facts wrong so feel free to contact me if you think some of these portions need to be corrected and it would be good if you could provide me appropriate references for these corrections deep learning so let us start with the first chapter which is on biological neurons as i said its spans several fields will start with biology and we will first talk about the brain and neurons within the brainso  way back in one87one one873 around that time joseph von gerlach actually proposed that the nervous system our nervous system is a single continuous network as opposed to a network of many discrete cells so his idea was that this is one gigantic cell sitting in our nervous system and it is not a network of discrete cells and this theory was known as the reticular theory and around the same time there was the some breakthrough or some progress in staining techniques where camillo golgi discovered that a chemical reaction that would allow you to examine the neurons or the nervous tissue so he was looking at this nervous tissue using some staining technique and by looking at what you see in this figure on the right hand side the yellow figure even he concluded that this is just once single cell and not a network of discrete cells so he was again a proponent of reticular theory so this is about camillo golgi and then interestingly santiago cajal he used the same technique which golgi proposed and he studied the same tissue and he came up with the conclusion that this is not a single cell this is actually a collection of various discrete cells which together forms a network so it is a network of things as opposed to a single cell there so that is what his theory was and this was eventually came to be known as the neuron doctrine although this was not a consolidated in the form of a doctrine by cajal that was done by this gentleman so he coined the term neuron so now today when you think about art here about artificial neural networks or artificial neurons the term neuron actually originated way back in one89one and this gentleman was responsible for coining that and he was also responsible for consolidating the neuron doctrine so already as you saw on the previous slide cajal had proposed it but then over the years many people bought this idea and this guy was responsible for consolidating that into a neuron doctrine interestingly he is not only responsible for coining the term neuron he is also responsible for coining the term chromosome so two very important terms were coined by this one person so now here is a question so around one906 when it was time to give the nobel prize in medicine what do you think which of these two proponents say there are two theories one is reticular theory which is a single cell and then there is this neuron doctrine which is a collection of cells or collection of neurons that a nervous system is a collection of neurons so what do you think which of these two guys who are proponents of these two different theories who would have got the actual nobel prize for medicine so interestingly it was given to both of them so till one906 in fact way later till one950 also this debate was not completely set settled and then the committee said both of these are interesting pieces of work we yet do not know what really actual what the situation is actually but these conflicting ideas have a place together and so the nobel prize was actually given to both of them and this led to a history of a some kind of controversies between these two scientists and so on and this debate surprisingly was settled way later in one950 and not by progress in biology as such but by progress in a different field so this was with the advent of electron microscopy so now it was able to see this at a much better scale and by looking at this under a microscope it was found that actually there is a gap between these neurons and hence it is not a one single cell it is actually a collection or a network of cells with a clear gap between them or some connections between them which are now known as synapses so this was when the debate was settled so now why am i talking about biology why am i telling you about biological neuron and so on so this is what we need to understand so there has always been interested in understanding how the human brain works from a biological perspective at least and around this time the debate was more or less settled that we have this our brain is a collection of many neurons and they interact with each other to help us do a lot of complex processing that we do on a daily basis right from getting up in the morning and deciding what do we want to do today taking decisions performing computations and various complex things that our brain is capable of doing now the interest is in seeing if we understand how the brain works can we make an artificial model for that so can we come up with something which can simulate how our brain works and what is that model and how do we make a computer do that or how do we make a machine do that so that is why i started from biological neurons to take the inspiration from biology "}
